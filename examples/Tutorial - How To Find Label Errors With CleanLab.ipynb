{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2021-2022 Cleanlab Inc.\n",
    "# This file is part of cleanlab/label-errors.\n",
    "#\n",
    "# label-errors is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# cleanlab/label-errors is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "\n",
    "# This agreement applies to this version and all previous versions of\n",
    "# cleanlab/label-errors.\n",
    "\n",
    "\"\"\"\n",
    "This tutorial provides reproducible code to find the label errors for datasets:\n",
    "MNIST, CIFAR-10, CIFAR-100, ImageNet, Caltech-256, Amazon Reviews, IMDB,\n",
    "20News, and AudioSet. These datasets comprise 9 of the 10 datasets on\n",
    "https://labelerrors.com .\n",
    "\n",
    "Label errors are found using the pred_probs (predicted probs), pred (predicted labels),\n",
    "and test label files, provided in this repo: (cleanlab/label-errors)\n",
    "\n",
    "The QuickDraw dataset is excluded because the pred_probs file is 33GB and might\n",
    "cause trouble on some machines. To find label errors in the QuickDraw dataset,\n",
    "you can download the pred_probs file here:\n",
    "https://github.com/cleanlab/label-errors/releases/tag/quickdraw-pred_probs-v1\n",
    "\n",
    "This tutorial reproduces how we find the label errors on https://labelerrors.com\n",
    "(prior to human validation on mTurk).\n",
    "\n",
    "To more closely match the label errors on labelerrors.com and in the paper,\n",
    "set reproduce_labelerrors_dot_com = True\n",
    "\"\"\"\n",
    "\n",
    "#import cleanlab  # this notebook requires cleanlab v1.0: https://pypi.org/project/cleanlab/1.0/ \n",
    "import cleanlab  # this notebook requires cleanlab v2.2.0\n",
    "import numpy as np\n",
    "import json\n",
    "from util import ALL_CLASSES\n",
    "# To view the text data from labelerrors.com, we need:\n",
    "from urllib.request import urlopen\n",
    "# To view the image data from labelerrors.com, we need:\n",
    "from skimage import io\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Remove axes since we're plotting images, not graphs\n",
    "rc = {\"axes.spines.left\" : False,\n",
    "      \"axes.spines.right\" : False,\n",
    "      \"axes.spines.bottom\" : False,\n",
    "      \"axes.spines.top\" : False,\n",
    "      \"xtick.bottom\" : False,\n",
    "      \"xtick.labelbottom\" : False,\n",
    "      \"ytick.labelleft\" : False,\n",
    "      \"ytick.left\" : False}\n",
    "plt.rcParams.update(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanlab.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    ('mnist_test_set', 'image'),\n",
    "    ('cifar10_test_set', 'image'),\n",
    "    ('cifar100_test_set', 'image'),\n",
    "    ('caltech256', 'image'),\n",
    "    #('imagenet_val_set', 'image'),\n",
    "    ('20news_test_set', 'text'),\n",
    "    #('imdb_test_set', 'text'),\n",
    "    #('amazon', 'text'),\n",
    "    ('audioset_eval_set', 'audio'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the filename mappings for various datasets\n",
    "root_dir = \"../dataset_indexing\"\n",
    "with open(root_dir + \"/audioset_eval_set_index_to_youtube_id.json\", 'r') as rf:\n",
    "    AUDIOSET_INDEX_TO_YOUTUBE = json.load(rf)\n",
    "with open(root_dir + \"/imdb_test_set_index_to_filename.json\", 'r') as rf:\n",
    "    IMDB_INDEX_TO_FILENAME = json.load(rf)\n",
    "with open(root_dir + \"/imagenet_val_set_index_to_filepath.json\", 'r') as rf:\n",
    "    IMAGENET_INDEX_TO_FILEPATH = json.load(rf)\n",
    "with open(root_dir + \"/caltech256_index_to_filename.json\", \"r\") as rf:\n",
    "    CALTECH256_INDEX_TO_FILENAME = json.load(rf)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_error_indices_to_match_labelerrors_com():\n",
    "    \"\"\"This method will reproduce the label errors found on labelerrors.com and\n",
    "    match (within a few percentage) the counts of label errors in Table 1 in the\n",
    "    label errors paper: https://arxiv.org/abs/2103.14749\n",
    "    \n",
    "    While reproducibility is nice, some of these methods have been improved, and\n",
    "    if you are not reproducing the results in the paper, we recommend using the\n",
    "    latest version of `cleanlab.pruning.get_noise_indices()`\n",
    "\n",
    "    Variations in method is due to the fact that this research was\n",
    "    conducted over the span of years. All methods use variations of\n",
    "    confident learning.\"\"\"\n",
    "\n",
    "    if dataset == 'imagenet_val_set':\n",
    "        cj = cleanlab.count.compute_confident_joint(\n",
    "            labellabels=labels, pred_probs=pred_probs, calibrate=False, )\n",
    "        num_errors = cj.sum() - cj.diagonal().sum()\n",
    "    elif dataset == 'mnist_test_set':\n",
    "        cj = cleanlab.count.compute_confident_joint(\n",
    "            labels=labels, pred_probs=pred_probs, calibrate=False, )\n",
    "        label_errors_bool = cleanlab.filter.find_label_issues(\n",
    "            labels=labels, pred_probs=pred_probs, confident_joint=cj, prune_method='prune_by_class',\n",
    "        )\n",
    "        num_errors = sum(label_errors_bool)\n",
    "    elif dataset != 'audioset_eval_set':  # Audioset is special case: it is multi-label\n",
    "        cj = cleanlab.count.compute_confident_joint(\n",
    "            labels=labels, pred_probs=pred_probs, calibrate=False, )\n",
    "        num_errors = cleanlab.count.num_label_issues(\n",
    "            labellabels=labels, pred_probs=pred_probs, confident_joint=cj, )\n",
    "    \n",
    "    if dataset == 'audioset_eval_set':  # Special case (multi-label) (TODO: update)\n",
    "        label_error_indices = cleanlab.filter.find_label_issues(\n",
    "            labels=labels, pred_probs=pred_probs, multi_label=True,\n",
    "            sorted_index_method='self_confidence', )\n",
    "        label_error_indices = label_error_indices[:307]\n",
    "    else:\n",
    "        prob_label = np.array([pred_probs[i, l] for i, l in enumerate(labels)])\n",
    "        max_prob_not_label = np.array(\n",
    "            [max(np.delete(pred_probs[i], l, -1)) for i, l in enumerate(labels)])\n",
    "        normalized_margin = prob_label - max_prob_not_label\n",
    "        label_error_indices = np.argsort(normalized_margin)[:num_errors]\n",
    "\n",
    "    return label_error_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find label errors in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, the code below will use the most up-to-date theory and algorithms\n",
    "# of confident learning, implemented in the cleanlab package.\n",
    "# We recommend this for best results.\n",
    "# However, if you need to more closely match the label errors \n",
    "# to match https://labelerrors.com, set `reproduce_labelerrors_dot_com = True`.\n",
    "# There may be discrepancies in counts due to improvements to cleanlab\n",
    "# since the work was published.\n",
    "\n",
    "# Set to False for best/most-recent results (this approach also runs faster)\n",
    "# Set to True to match the label errors on https://labelerrors.com\n",
    "reproduce_labelerrors_website = False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================\n",
      "Dataset: Mnist_test_set\n",
      "=======================\n",
      "Finding label errors using cleanlab for 10,000 examples and 10 classes...\n",
      "Estimated number of errors in mnist_test_set: 15\n",
      " * Mnist_test_set Given Label: 5\n",
      " * We Guess (argmax prediction): 3\n",
      " * Label Error Found: https://labelerrors.com/static/mnist/2597.png\n",
      "\n",
      "=========================\n",
      "Dataset: Cifar10_test_set\n",
      "=========================\n",
      "Finding label errors using cleanlab for 10,000 examples and 10 classes...\n",
      "Estimated number of errors in cifar10_test_set: 284\n",
      " * Cifar10_test_set Given Label: automobile\n",
      " * We Guess (argmax prediction): airplane\n",
      " * Label Error Found: https://labelerrors.com/static/cifar10/3828.png\n",
      "\n",
      "==========================\n",
      "Dataset: Cifar100_test_set\n",
      "==========================\n",
      "Finding label errors using cleanlab for 10,000 examples and 100 classes...\n",
      "Estimated number of errors in cifar100_test_set: 2120\n",
      " * Cifar100_test_set Given Label: clock\n",
      " * We Guess (argmax prediction): fox\n",
      " * Label Error Found: https://labelerrors.com/static/cifar100/9471.png\n",
      "\n",
      "===================\n",
      "Dataset: Caltech256\n",
      "===================\n",
      "Finding label errors using cleanlab for 29,780 examples and 256 classes...\n",
      "Estimated number of errors in caltech256: 2344\n",
      " * Caltech256 Given Label: yo-yo\n",
      " * We Guess (argmax prediction): frisbee\n",
      " * Label Error Found: https://labelerrors.com/static/caltech256/249.yo-yo/249_0025.jpg\n",
      "\n",
      "========================\n",
      "Dataset: 20news_test_set\n",
      "========================\n",
      "Finding label errors using cleanlab for 7,532 examples and 20 classes...\n",
      "Estimated number of errors in 20news_test_set: 95\n",
      " * 20news_test_set Given Label: comp.os.ms-windows.misc\n",
      " * We Guess (argmax prediction): comp.graphics\n",
      " * Label Error Found: https://labelerrors.com/static/20news/3039.txt\n",
      "\n",
      "==========================\n",
      "Dataset: Audioset_eval_set\n",
      "==========================\n",
      "(20371,)\n",
      "Finding label errors using cleanlab for 20,371 examples and 527 classes...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Find label error indices using cleanlab in one line of code. \u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# This will use the most recent version of cleanlab with best results.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinding label errors using cleanlab for \u001b[39m\u001b[38;5;132;01m{:,}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     32\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexamples and \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m classes...\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39mpred_probs\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m---> 33\u001b[0m     label_error_indices \u001b[38;5;241m=\u001b[39m \u001b[43mcleanlab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_label_issues\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpred_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Try prune_method='both' (C+NR in the confident learning paper)\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# 'both' finds fewer errors, but can sometimes increase precision\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilter_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprune_by_noise_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maudioset_eval_set\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_indices_ranked_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mself_confidence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m num_errors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(label_error_indices)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEstimated number of errors in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(dataset), num_errors)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/cleanlab/filter.py:222\u001b[0m, in \u001b[0;36mfind_label_issues\u001b[0;34m(labels, pred_probs, return_indices_ranked_by, rank_by_kwargs, filter_by, multi_label, frac_noise, num_to_remove_per_class, min_examples_per_class, confident_joint, n_jobs, verbose)\u001b[0m\n\u001b[1;32m    220\u001b[0m allow_one_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(labels, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(lab, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m lab \u001b[38;5;129;01min\u001b[39;00m labels):\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m {\u001b[38;5;241m0\u001b[39m}:  \u001b[38;5;66;03m# occurs with missing classes in multi-label settings\u001b[39;00m\n\u001b[1;32m    223\u001b[0m         allow_one_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    224\u001b[0m assert_valid_inputs(\n\u001b[1;32m    225\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    226\u001b[0m     y\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m     allow_one_class\u001b[38;5;241m=\u001b[39mallow_one_class,\n\u001b[1;32m    230\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "for (dataset, modality) in datasets:\n",
    "    title = 'Dataset: ' + dataset.capitalize()\n",
    "    print('='*len(title), title, '='*len(title), sep='\\n')\n",
    "    \n",
    "    # Get the cross-validated predicted probabilities on the test set.\n",
    "    if dataset == 'amazon' or dataset == 'imagenet_val_set':\n",
    "        n_parts = 3 if dataset == 'amazon' else 4\n",
    "        pred_probs_fn = '../cross_validated_predicted_probabilities/' \\\n",
    "             '{}_pyx.part{}_of_{}.npy'\n",
    "        parts = [np.load(pred_probs_fn.format(dataset, i + 1, n_parts)) for i in range(n_parts)]\n",
    "        pred_probs = np.vstack(parts)\n",
    "        if dataset == 'imagenet_val_set':\n",
    "            print(f\"imagenet val, max prob = {np.max(pred_probs)}, min={np.min(pred_probs)}\")\n",
    "    else:\n",
    "        pred_probs = np.load('../cross_validated_predicted_probabilities/' \\\n",
    "            '{}_pyx.npy'.format(dataset), allow_pickle=True)\n",
    "    # Get the cross-validated predictions (argmax of pred_probs) on the test set.\n",
    "    pred = np.load('../cross_validated_predicted_labels/'\n",
    "        '{}_pyx_argmax_predicted_labels.npy'.format(dataset), allow_pickle=True)\n",
    "    # Get the test set labels\n",
    "    labels = np.load('../original_test_labels/'\n",
    "        '{}_original_labels.npy'.format(dataset), allow_pickle=True)\n",
    "    if dataset == \"audioset_eval_set\":\n",
    "        print(labels.shape)\n",
    "    \n",
    "    if reproduce_labelerrors_website:\n",
    "        label_error_indices = get_label_error_indices_to_match_labelerrors_com()\n",
    "    else:\n",
    "        # Find label error indices using cleanlab in one line of code. \n",
    "        # This will use the most recent version of cleanlab with best results.\n",
    "        print('Finding label errors using cleanlab for {:,} '\n",
    "              'examples and {} classes...'.format(*pred_probs.shape))\n",
    "        label_error_indices = cleanlab.filter.find_label_issues(\n",
    "            labels=labels,\n",
    "            pred_probs=pred_probs,\n",
    "            # Try prune_method='both' (C+NR in the confident learning paper)\n",
    "            # 'both' finds fewer errors, but can sometimes increase precision\n",
    "            filter_by='prune_by_noise_rate',\n",
    "            multi_label=True if dataset == 'audioset_eval_set' else False,\n",
    "            return_indices_ranked_by='self_confidence',\n",
    "        )\n",
    "    num_errors = len(label_error_indices)\n",
    "    print('Estimated number of errors in {}:'.format(dataset), num_errors)\n",
    "    \n",
    "    # Print an example\n",
    "    # Grab the first label error found with cleanlab\n",
    "    err_id = label_error_indices[0]\n",
    "    \n",
    "    # Custom code to visualize each label error from each dataset\n",
    "    dname = dataset.split('_')[0]  # Get dataset name\n",
    "    url_base = \"https://labelerrors.com/static/{}/{}\".format(dname, err_id)\n",
    "    if modality == 'image':\n",
    "        if dataset == 'imagenet_val_set':\n",
    "            image_path = IMAGENET_INDEX_TO_FILEPATH[err_id]\n",
    "            url = url_base.replace(str(err_id), image_path)\n",
    "        elif dataset == 'caltech256':\n",
    "            image_path = CALTECH256_INDEX_TO_FILENAME[err_id]\n",
    "            url = url_base.replace(str(err_id), image_path)\n",
    "        else:\n",
    "            url = url_base + \".png\"\n",
    "        #image = io.imread(url)  # read image data from a url\n",
    "        #plt.imshow(image, interpolation='nearest', aspect='auto', cmap='gray')\n",
    "        #plt.show()\n",
    "    elif modality == 'text':\n",
    "        if dataset == 'amazon':\n",
    "            # There are 400,000+ amazon reviews errors -- we only check a small\n",
    "            # fraction on labelerrors.com, so choose one that's on the website.\n",
    "            err_id = 8864504\n",
    "            assert err_id in label_error_indices  # Did we find this error?\n",
    "            url = \"https://labelerrors.com/static/{}/{}.txt\".format(dname, err_id)\n",
    "        elif dataset == 'imdb_test_set':\n",
    "            imdb_fn = IMDB_INDEX_TO_FILENAME[err_id]  \n",
    "            url = \"https://labelerrors.com/static/{}/test/{}\".format(dname, imdb_fn)\n",
    "        else:\n",
    "            url = url_base + \".txt\"\n",
    "        #text = urlopen(url).read().decode(\"utf-8\")  # read raw text from a url\n",
    "        #print('\\n{} Text Example (ID: {}):\\n{}\\n'.format(\n",
    "        #    dataset.capitalize(), err_id, text))\n",
    "    elif modality == 'audio':  # dataset == 'audioset_eval_set'\n",
    "        # Because AudioSet is multi-label, we only look at examples where the \n",
    "        # predictions have no overlap with the labels to avoid overcounting.\n",
    "        label_error_indices = [z for z in label_error_indices \\\n",
    "                if set(pred[z]).intersection(labels[z]) == set()]\n",
    "        err_id = label_error_indices[1]\n",
    "        youtube_id = AUDIOSET_INDEX_TO_YOUTUBE[err_id]\n",
    "        #url = youtube_id.replace('http', 'https')\n",
    "    # Map label indices to class names\n",
    "    if dataset == 'audioset_eval_set':  # multi-label    \n",
    "        given_label = [ALL_CLASSES[dataset][z] for z in labels[err_id]]\n",
    "        pred_label = [ALL_CLASSES[dataset][z] for z in pred[err_id]]\n",
    "    else:  # single-label\n",
    "        given_label = ALL_CLASSES[dataset][labels[err_id]]\n",
    "        pred_label = ALL_CLASSES[dataset][pred[err_id]]\n",
    "    print(' * {} Given Label:'.format(dataset.capitalize()), given_label)\n",
    "    print(' * We Guess (argmax prediction):', pred_label)\n",
    "    print(' * Label Error Found: {}\\n'.format(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([195, 196], dtype=uint16), array([0, 5], dtype=uint16),\n",
       "       array([ 13,  66,  69, 504], dtype=uint16), ...,\n",
       "       array([177], dtype=uint16),\n",
       "       array([300, 335, 336, 340], dtype=uint16),\n",
       "       array([137, 183], dtype=uint16)], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
